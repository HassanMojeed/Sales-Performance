{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python script by [__Hassan Mojeed__](https://linktr.ee/mhola)<br>\n",
    "Email: mojeed.o.hassan@gmail.com<br>\n",
    "[Linkedin Profile](https://www.linkedin.com/in/hassanmojeed/)\n",
    "\n",
    "\n",
    "\n",
    "## View Dashbaord [here](https://app.powerbi.com/view?r=eyJrIjoiOTBiODkwYTYtMjk2MC00MzMzLWJkOGYtZWQzNGFiNTdjODZkIiwidCI6ImFlM2E5OTA2LTc4MWEtNDQ2YS1iZGI2LTYzNzdjMDllMmM2ZiIsImMiOjF9&pageName=ReportSectioneed2047ccf3ea2f7d6e8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction:\n",
    "The Python code will extract, clean, and load sales data from Jumia Nigeria's sales department into a Microsoft SQL Server database. It will utilize various libraries and handle data processing tasks for sales data from March 2018.\n",
    "\n",
    "#### Aim:\n",
    "The objective is to analyze the sales performance of the Jumia Nigeria sales team for the specified period. This will involve extracting  relevant sales data, ensuring consistency and accuracy through cleaning, and finally loading it into an MSSQL Server database for further analysis.\n",
    "\n",
    "#### Data Source:\n",
    "Excel files stored in a directory named \"Jumia\" will serve as the data source. The code will locate these files using the glob function within the specified directory.\n",
    "\n",
    "#### Data Extraction:\n",
    "Using pd.read_excel(), the code will read data from the second Excel file found in the \"Jumia\" directory into a Pandas DataFrame.\n",
    "\n",
    "#### Data Cleaning and Manipulation:\n",
    "\n",
    "Column Removal: Unnecessary columns will be dropped from the DataFrame.<br><br>\n",
    "Column Renaming: Columns will be renamed for clarity and consistency.<br><br>\n",
    "Handling Missing Values: Missing values in specific columns ('Level', 'Region', and 'sub_category') will be filled with 'Unknown', and conditional replacements will be made based on certain criteria.<br><br>\n",
    "Value Replacements: Various columns such as 'Region', 'category', 'sub_category', 'brand', and 'item_status' will undergo value replacements to ensure consistency and accuracy.<br><br>\n",
    "Custom Function Application: The custom function replace_col1_with_col2 will be applied to perform specific replacements in the 'category' column.<br><br>\n",
    "New Column Creation: A new column named 'final_delivered_date' will be created based on certain conditions, and additional columns will be created to calculate achieved, shipped, and delivered revenues.<br><br>\n",
    "Data Type Conversion: Certain columns will be converted to appropriate data types.<br><br>\n",
    "Date Formatting: Date-related columns will be formatted to the desired format.<br>\n",
    "\n",
    "#### Data Export:\n",
    "After processing, the DataFrame will be exported to an MSSQL database named 'JumiaDB_Project' and stored in a table named 'Sales_data'. The code will establish a connection to the database using PyODBC and SQLAlchemy, and then export the DataFrame to the specified table. A success message will be printed upon successful export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import socket\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import pyodbc\n",
    "import sqlalchemy\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings to enhance readability\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/mj/Projects/Projects/sales_analysis/Jumia/~$Sales Data.xlsx',\n",
       " '/Users/mj/Projects/Projects/sales_analysis/Jumia/Sales Data.xlsx']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the current working directory and find Excel files in the \"Jumia\" folder\n",
    "pwd = os.getcwd() + \"/Jumia\"\n",
    "file = glob(pwd + \"/*.xlsx\")\n",
    "\n",
    "# Display the list of Excel files found\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the second Excel file into a pandas DataFrame\n",
    "data_import = pd.read_excel(file[1], index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31864 entries, 0 to 31863\n",
      "Data columns (total 33 columns):\n",
      " #   Column                         Non-Null Count  Dtype         \n",
      "---  ------                         --------------  -----         \n",
      " 0   Level                          31499 non-null  object        \n",
      " 1   Region                         31484 non-null  object        \n",
      " 2   year_month                     31864 non-null  object        \n",
      " 3   year_week                      31864 non-null  object        \n",
      " 4   date                           31864 non-null  datetime64[ns]\n",
      " 5   order_nr                       31864 non-null  int64         \n",
      " 6   payment_method                 31864 non-null  object        \n",
      " 7   item_status                    31864 non-null  object        \n",
      " 8   customer_device                31864 non-null  object        \n",
      " 9   brand                          31861 non-null  object        \n",
      " 10  category_l2                    31505 non-null  object        \n",
      " 11  category_l1                    31864 non-null  object        \n",
      " 12  article_name                   31864 non-null  object        \n",
      " 13  SKU                            31864 non-null  object        \n",
      " 14  pc1_value_incl_tax             29354 non-null  float64       \n",
      " 15  paid_price_incl_tax            31864 non-null  float64       \n",
      " 16  coupon_code                    16174 non-null  object        \n",
      " 17  coupon_value_incl_tax          31864 non-null  float64       \n",
      " 18  selling_price_incl_tax         31864 non-null  float64       \n",
      " 19  customer_type                  31864 non-null  object        \n",
      " 20  id_customer                    31864 non-null  int64         \n",
      " 21  customer_email                 31864 non-null  object        \n",
      " 22  order_placed_by                110 non-null    object        \n",
      " 23  Jforce                         31864 non-null  object        \n",
      " 24  bob_id_sales_order_item        31864 non-null  int64         \n",
      " 25  bi_shipped_date                25877 non-null  object        \n",
      " 26  bi_delivered_date              9616 non-null   object        \n",
      " 27  bi_delivered_final_date        1137 non-null   object        \n",
      " 28  bi_rejected_date               1231 non-null   object        \n",
      " 29  DSC_BI_PRODUCT_CATEGORY_ONE    31688 non-null  object        \n",
      " 30  DSC_BI_PRODUCT_CATEGORY_TWO    0 non-null      float64       \n",
      " 31  DSC_BI_PRODUCT_CATEGORY_THREE  0 non-null      float64       \n",
      " 32  mv_nnn                         0 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(7), int64(3), object(22)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the read DataFrame\n",
    "data = data_import.copy()\n",
    "\n",
    "\n",
    "# Display information about the data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Level                              365\n",
       "Region                             380\n",
       "year_month                           0\n",
       "year_week                            0\n",
       "date                                 0\n",
       "order_nr                             0\n",
       "payment_method                       0\n",
       "item_status                          0\n",
       "customer_device                      0\n",
       "brand                                3\n",
       "category_l2                        359\n",
       "category_l1                          0\n",
       "article_name                         0\n",
       "SKU                                  0\n",
       "pc1_value_incl_tax                2510\n",
       "paid_price_incl_tax                  0\n",
       "coupon_code                      15690\n",
       "coupon_value_incl_tax                0\n",
       "selling_price_incl_tax               0\n",
       "customer_type                        0\n",
       "id_customer                          0\n",
       "customer_email                       0\n",
       "order_placed_by                  31754\n",
       "Jforce                               0\n",
       "bob_id_sales_order_item              0\n",
       "bi_shipped_date                   5987\n",
       "bi_delivered_date                22248\n",
       "bi_delivered_final_date          30727\n",
       "bi_rejected_date                 30633\n",
       "DSC_BI_PRODUCT_CATEGORY_ONE        176\n",
       "DSC_BI_PRODUCT_CATEGORY_TWO      31864\n",
       "DSC_BI_PRODUCT_CATEGORY_THREE    31864\n",
       "mv_nnn                           31864\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the data\n",
    "data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Functions\n",
    "\n",
    "def replace_col1_with_col2(data, column_one, column_two, text_to_replace):\n",
    "    \"\"\"\n",
    "    Replace values in 'column_one' with values in 'column_two'\n",
    "    where 'column_one' equals 'text_to_replace'.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame\n",
    "    - column_one: str, name of the column to be replaced\n",
    "    - column_two: str, name of the column with replacement values\n",
    "    - text_to_replace: str, value to be replaced in 'column_one'\n",
    "    \n",
    "    Returns:\n",
    "    - Modified 'column_one' in the DataFrame\n",
    "    \"\"\"\n",
    "    def switch_one_text(row):\n",
    "        if row[column_one] == text_to_replace:\n",
    "            return row[column_two]\n",
    "        else:\n",
    "            return row[column_one]\n",
    "    \n",
    "    data[column_one] = data.apply(switch_one_text, axis=1)\n",
    "    return data[column_one].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be removed from the DataFrame\n",
    "remove_columns = ['order_placed_by', 'year_month', 'DSC_BI_PRODUCT_CATEGORY_TWO',\n",
    "                  'DSC_BI_PRODUCT_CATEGORY_THREE', 'mv_nnn', 'year_week']\n",
    "\n",
    "# Drop specified columns\n",
    "data.drop(columns=remove_columns, inplace=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "data.rename(columns={'category_l2': 'sub_category', 'category_l1': 'category', 'order_nr': 'order_number'},\n",
    "            inplace=True)\n",
    "\n",
    "#data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in specific columns with 'Unknown'\n",
    "data['Level'] = data['Level'].fillna('Unknown')\n",
    "data['Region'] = data['Region'].fillna('Unknown')\n",
    "data['sub_category'] = data['sub_category'].fillna('Unknown')\n",
    "\n",
    "# Conditional replacements based on certain criteria\n",
    "data['sub_category'] = data.apply(lambda row: 'Mobile Phones' \n",
    "                                        if row['category'] == 'Top Searched' \n",
    "                                            and row['sub_category'] == 'Unknown'\n",
    "                                        else row['sub_category'], axis =1)\n",
    "\n",
    "data['sub_category'] = data.apply(lambda row: 'Mobile Phones' \n",
    "                                        if row['brand'] == 'Leagoo'\n",
    "                                            and row['sub_category'] == 'Unknown'\n",
    "                                        else row['sub_category'], axis =1)\n",
    "\n",
    "data['sub_category'] = data.apply(lambda row: 'Peripherals & Accessories' \n",
    "                                        if row['brand'] == 'Universal'\n",
    "                                            and row['sub_category'] == 'Unknown' \n",
    "                                            and row['DSC_BI_PRODUCT_CATEGORY_ONE'] == 'Phones'\n",
    "                                        else row['sub_category'], axis = 1)\n",
    "\n",
    "data['sub_category'] = data.apply(lambda row: 'Packaging' \n",
    "                                        if row['sub_category'] == 'Unknown'\n",
    "                                            and row['category'] == 'Vendor Packaging'\n",
    "                                        else row['sub_category'], axis = 1)\n",
    "\n",
    "data['sub_category'] = data.apply(lambda row: 'Computing' \n",
    "                                        if row['sub_category'] == 'Computing'\n",
    "                                            and row['category'] == 'Clearance Sales'\n",
    "                                        else row['sub_category'], axis = 1)\n",
    "\n",
    "data['brand'] = data.apply(lambda row: 'Crown' \n",
    "                                        if row['brand'] == 'Crown Micro'\n",
    "                                            and row['category'] == 'TVs & Electronics'\n",
    "                                        else row['brand'], axis = 1)\n",
    "\n",
    "data['brand'] = data.apply(lambda row: 'Garrett' \n",
    "                                        if row['brand'] == 'Garret'\n",
    "                                            and row['category'] == 'TVs & Electronics'\n",
    "                                        else row['brand'], axis = 1)\n",
    "\n",
    "# Replacements for specific values in 'Region', 'category' 'sub_category', 'brand' and 'item_status' columns\n",
    "data['Region'].replace({'SW Region-Lagos' : 'Lagos', \n",
    "                        'SS Region': 'South-South', \n",
    "                        'SE Region': 'South-East',\n",
    "                        'SW Region-Others': 'South-West'},\n",
    "                        inplace=True\n",
    "                        )\n",
    "\n",
    "data['category'].replace({'Clearance Sale!': 'Clearance Sales', \n",
    "                        'Fashion Outlet': 'Fashion',\n",
    "                        'Top Searched': 'Phones & Tablets'}, \n",
    "                        inplace=True\n",
    "                        )\n",
    "\n",
    "data['sub_category'].replace({'Video Games': 'Video Games & Consoles',\n",
    "                              'All Watches': 'Watches',\n",
    "                              'Blow Out Sales': 'Sunglasses',\n",
    "                              'Bridal Shower': 'Brides',\n",
    "                              'Sony Computer Entertainment': 'Sony',\n",
    "                              'Clearance Sale GM': 'Clearance Sale',\n",
    "                              'Computing': 'Laptops'\n",
    "                              },\n",
    "                        inplace=True\n",
    "                        )\n",
    "\n",
    "data['brand'].replace({'Newage': 'New Age',\n",
    "                       'Aimegao': 'Aimeigao',\n",
    "                       'Ameigao': 'Aimeigao',\n",
    "                       'AMG - AIMEIGAO': 'Aimeigao',\n",
    "                       'ALBERTO': 'Alberto VO5',\n",
    "                       'Asantee': 'Asantee Herbal',\n",
    "                       'Barbara': 'Barbara Fashion',\n",
    "                       'Brazillian Hair': 'Brazilian Hair',\n",
    "                       'Cardinal': 'Cardinal Polo Club',\n",
    "                       'Carossi': 'Carossi Shoe',\n",
    "                       \"Carter's little layette\": \"Carter's\",\n",
    "                       'DeYoung': 'De Young',\n",
    "                       'Dynamic': 'Dynamic Collection',\n",
    "                       'EA': 'EA Sports',\n",
    "                       'Ecostyler': 'Eco Styler',\n",
    "                       'eGO CE4': 'EGO',\n",
    "                       'Frank Olivier': 'Franck Olivier',\n",
    "                       'Haier': 'Haier Thermocool',\n",
    "                       'HpPower': 'HP',\n",
    "                       'HP Power': 'HP',\n",
    "                       'Indian': 'Indian Hair',\n",
    "                       'Jacobs': \"Jacob's\",\n",
    "                       'Pleasers USA': 'Pleaser USA',\n",
    "                       'QLT Headsets': 'QLT',\n",
    "                       'QLT Choice': 'QLT',\n",
    "                       'SheaMoisture': 'Shea Moisture',\n",
    "                       'TP Link': 'TP-Link',\n",
    "                       'Zaron': 'Zaron Cosmetics'\n",
    "                        },\n",
    "                        inplace=True\n",
    "                        )\n",
    "\n",
    "data['item_status'].replace({'delivered_final': 'delivered'\n",
    "                             }, \n",
    "                        inplace=True\n",
    "                        )\n",
    "\n",
    "# Apply custom replacements using the 'replace_col1_with_col2' function\n",
    "replace_col1_with_col2(data, \n",
    "                column_one='category',\n",
    "                column_two='sub_category', \n",
    "                text_to_replace='Shipped From Overseas'\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'final_delivered_date' based on conditions\n",
    "\n",
    "data['final_delivered_date'] = data.apply(lambda row: row['bi_delivered_date'] \n",
    "                                          if pd.isna(row['bi_delivered_final_date']) \n",
    "                                          else row['bi_delivered_final_date'], axis=1)\n",
    "\n",
    "# Achieved revenue: All orders with item status except for \"Invalid\" and \"Fraud\"\n",
    "data['Achieved revenue'] = data.apply(lambda row: np.nan \n",
    "                                      if row['item_status'] in ['Invalid', 'Fraud'] \n",
    "                                      else row['paid_price_incl_tax'], axis=1).astype(float)\n",
    "\n",
    "# Shipped revenue: All orders with Bi shipped date\n",
    "data['Shipped revenue'] = data.apply(lambda row: np.nan \n",
    "                                     if pd.isna(row['bi_shipped_date']) \n",
    "                                     else row['paid_price_incl_tax'], axis=1).astype(float)\n",
    "\n",
    "# Delivered revenue: All orders with Item status 'Delivered' and have 'Bi delivered date'.\n",
    "data['Delivered revenue']  = data.apply(lambda row: row['paid_price_incl_tax']\n",
    "                                        if row['item_status'] == 'delivered' \n",
    "                                            and pd.notna(row['final_delivered_date']) \n",
    "                                        else 0, axis =1).astype(float)\n",
    "\n",
    "# Clean and format column names\n",
    "columns = data.columns.str.split('_').str.join(' ').str.title()\n",
    "data.columns = columns.str.split(\" \").str.join(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert specific columns to the appropriate data types\n",
    "data['Paid_Price_Incl_Tax']=data['Paid_Price_Incl_Tax'].astype(float)\n",
    "\n",
    "data['Coupon_Value_Incl_Tax']= data['Coupon_Value_Incl_Tax'].astype(float)\n",
    "\n",
    "data['Selling_Price_Incl_Tax']=data['Selling_Price_Incl_Tax'].astype(float)\n",
    "\n",
    "\n",
    "# Extract and format date-related columns\n",
    "data['Year_Month'] =pd.to_datetime(data['Date']).dt.strftime('%Y-%m')\n",
    "\n",
    "data['Year_Week'] =pd.to_datetime(data['Date']).dt.strftime('%G-%V')\n",
    "\n",
    "data['Bi_Delivered_Date']=pd.to_datetime(data['Bi_Delivered_Date']).dt.date\n",
    "\n",
    "data['Bi_Delivered_Final_Date']=pd.to_datetime(data['Bi_Delivered_Final_Date']).dt.date\n",
    "\n",
    "data['Bi_Rejected_Date']=pd.to_datetime(data['Bi_Rejected_Date']).dt.date\n",
    "\n",
    "data['Final_Delivered_Date']=pd.to_datetime(data['Final_Delivered_Date']).dt.date\n",
    "\n",
    "data['Date']=pd.to_datetime(data['Date']).dt.date\n",
    "\n",
    "data['Bi_Shipped_Date']=pd.to_datetime(data['Bi_Shipped_Date']).dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SQL Server',\n",
       " 'MySQL ODBC 8.0 ANSI Driver',\n",
       " 'MySQL ODBC 8.0 Unicode Driver',\n",
       " 'SQL Server Native Client 11.0',\n",
       " 'SQL Server Native Client RDA 11.0',\n",
       " 'Microsoft Access Driver (*.mdb, *.accdb)',\n",
       " 'Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)',\n",
       " 'Microsoft Access Text Driver (*.txt, *.csv)',\n",
       " 'ODBC Driver 17 for SQL Server']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the available drivers\n",
    "pyodbc.drivers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales_data has been successfully exported to JumiaDB_Project MSSQL database\n"
     ]
    }
   ],
   "source": [
    "# Establish a connection to the MSSQL database\n",
    "database_name = 'JumiaDB_Project'\n",
    "table_name = 'Sales_data'\n",
    "socket_name =socket.gethostname()\n",
    "driver='ODBC Driver 17 for SQL Server'\n",
    "\n",
    "conn = sqlalchemy.create_engine(f'mssql+pyodbc://{socket_name}/{database_name}?trusted_connection=yes&driver={driver}')\n",
    "\n",
    "# Export the DataFrame to the MSSQL database\n",
    "data.to_sql(table_name, con = conn, if_exists='replace', index=False)\n",
    "\n",
    "# Print a success message\n",
    "print(f'{table_name} has been successfully exported to {database_name} MSSQL database')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [View exported data](https://drive.google.com/drive/folders/1_TFvO1MgM-I4aWpS2iArAlRys5A26oMg?usp=share_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sales_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
